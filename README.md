# Mini Project: Dynamic Programming

In this project, I wrote implementations of many classical dynamic programming algorithms.

### Part 0: Explore FrozenLakeEnv
In this section, I create an instance of the FrozenLake environment.

### Part 1: Iterative Policy Evaluation

In this section, I wrote my own implementation of iterative policy evaluation.

### Part 2: Obtain <img src="/tex/27b15edff3a616d929c11d17547d6df6.svg?invert_in_darkmode&sanitize=true" align=middle width=15.43827284999999pt height=14.15524440000002pt/> from <img src="/tex/143c6aa101ce0c82aab772be351df16b.svg?invert_in_darkmode&sanitize=true" align=middle width=16.06802669999999pt height=14.15524440000002pt/>

In this section, I worte a function that takes the state-value function estimate as input, along with some state <img src="/tex/a83a1e1ea9e8c0c1b1d2454514d4d2c6.svg?invert_in_darkmode&sanitize=true" align=middle width=38.98379759999999pt height=22.465723500000017pt/>.  It returns the **row in the action-value function** corresponding to the input state <img src="/tex/a83a1e1ea9e8c0c1b1d2454514d4d2c6.svg?invert_in_darkmode&sanitize=true" align=middle width=38.98379759999999pt height=22.465723500000017pt/>.  That is, my function accepts as input both <img src="/tex/143c6aa101ce0c82aab772be351df16b.svg?invert_in_darkmode&sanitize=true" align=middle width=16.06802669999999pt height=14.15524440000002pt/> and <img src="/tex/6f9bad7347b91ceebebd3ad7e6f6f2d1.svg?invert_in_darkmode&sanitize=true" align=middle width=7.7054801999999905pt height=14.15524440000002pt/>, and return <img src="/tex/f4fd307c043a8d1ae7d10dec8715ec8f.svg?invert_in_darkmode&sanitize=true" align=middle width=52.74613904999998pt height=24.65753399999998pt/> for all <img src="/tex/ceb7c02dd08156f7c48efa40c2d8dec1.svg?invert_in_darkmode&sanitize=true" align=middle width=62.396762999999986pt height=24.65753399999998pt/>.

### Part 3: Policy Improvement

In this section, I wrote my own implementation of policy improvement. 

### Part 4: Policy Iteration

In this section, I wrote my own implementation of policy iteration.  The algorithm returns the optimal policy, along with its corresponding state-value function.

### Part 5: Truncated Policy Iteration

In this section, I wrote my own implementation of truncated policy iteration.

### Part 6: Value Iteration

In this section, I wrote my own implementation of value iteration.
